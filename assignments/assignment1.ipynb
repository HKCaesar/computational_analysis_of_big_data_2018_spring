{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `computational_analysis_of_big_data_2018_spring` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, February 28, 2018, 23:59<br>\n",
    "**Peergrading deadline**: *Sunday*, March 4, 2018, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Wednesday*, March 7, 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Questions**](https://github.com/ulfaslak/computational_analysis_of_big_data_2018_spring/issues) **/** [**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 1.2.2**: Working with JSON files\n",
    ">1. Use [`requests`](https://www.google.dk/search?q=python+requests+get+json&gws_rd=cr&ei=M5OdWaewD8Ti6AS54J24Bg), or another Python module, to store **[this data](https://www.reddit.com/r/gameofthrones/.json)** in a new variable `data`. Store it as a `dict` type object (you can print its type by running `type(data)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.2.5**: Write two `for` loops (or list comprehensions for extra street credits) which:\n",
    ">1. Counts the number of spoilers.\n",
    ">2. Only prints headlines that aren't spoilers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 2.1.5**: Take another list `b = list(\"ofcourseistillloveyou\")` and\n",
    "1. get the `set` of characters that exist in both `a` and `b` (intersection),\n",
    "2. get the `set` of characters that exist in either `a` or `b` (union), and\n",
    "3. compute the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) between the distinct elements in `a` and `b`.\n",
    "\n",
    ">*Hint: use the `set` function to get a `set`-type object of distinct elements from a list*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: Looking at the scatter plots there appears to be some unevenness in the number of comments and upvotes that different posts receive.\n",
    "1. Plot the distributions of `x` for \"gameofthrones\" and \"news\" as histograms, side by side. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2c.png).\n",
    "2. What do these distributions say about how people comment on Reddit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.5**: Create a $5 \\times 5$ array `X` (a matrix) with random numbers and $5 \\times 1$ array `a` (a tall vector) with 5 random numbers.\n",
    "1. Compute the matrix-vector dot product between these two. Use `numpy`'s `dot` method. \n",
    "2. What happens if you just use `*` instead of `np.dot`? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.4.1**: Take a vector `a = [1, 3, 2, 5, 3, 1, 5, 1, 9000]`:\n",
    "1. Compute the mean of `a` using `numpy`.\n",
    "2. How is median defined? Compute the median of `a` using `numpy`.\n",
    "3. For `a`, why might it make sense to take the median more seriously than the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes' to get the characters in each category.\n",
    "1. How many superheroes are there? How many supervillains?\n",
    "2. How many characters are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character, and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    "\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. Use `plt.hist` on a list of page lengths, with the argument `normed=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: We are interested in knowing if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "* For each class, visualize the amount of characters introduced over time. You choose how you want to visualize this data, but please comment on your choice. Also comment on the outcome of your analysis.\n",
    "\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*\n",
    "\n",
    ">*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you had trouble boulding a team alliance matrix, use mine. You can load it as a pandas.DataFrame, with pd.read_csv('data_team_alliances.csv', index_col=0). The rightmost column is the target array.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.2.1**: Train a classifier on all of your data and test its accuracy.\n",
    "\n",
    ">* If your team alliance matrix is `X_ta` and your target array is `y_ta` you can do this by instantiating a model like:\n",
    ">\n",
    "        from sklearn.naive_bayes import BernoulliNB\n",
    "        model = BernoulliNB()\n",
    "        model.fit(X_ta, y_ta)  # <--- This is the training/fitting/learning step\n",
    "        \n",
    "> The `BernoulliNB` is a version of the Naive Bayes classifier which associates certain features with labels and asks what the probability of a label for a data point is given its features. You are free to use any other classifier if you want. Popular ones are trees, random forests, support vector machines, feed forward neural networks, logistic regression, and the list goes on. With `sklearn`, they are just as easy to employ as the `BernoulliNB` classifier.\n",
    "\n",
    "\n",
    ">1. Test the accuracy of your model. You can use the `.predict` method on the `model` object to get predictions for a matrix of data points. Report the accuracy of your model on the same data that you trained the model on, alongside the baseline accuracy of a \"dumb\" model that only guesses for the majority class.\n",
    "\n",
    ">2. Report the precision, recall and F1 scores, with respect to the minority class (heroes). `sklearn` has implementations that you can use if you are short for time. Extra credit for doing it using only basic linear algebra operations with `numpy`, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Feature representation\n",
    "In it's raw format, the data cannot be given to a machine learning algorithm. What we must do is extract features from the data and put them into a structured format. This is the same as what we did when we looked at a dog (the data) and extracted into a matrix whether it was fluffy, sad looking, etc. (the features). The feature we will extract here is **team alliances**.\n",
    "\n",
    "We can represent the team alliances of each character as a row in a matrix where each column corresponds to a particular team. That should look something like this (numbers are made up):\n",
    "\n",
    "<img src=\"http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_boa.png\" width=\"400\"/>\n",
    "\n",
    "**Note**: The following exercises relies on the dataset you produced in Ex. 3.1.2 (character markup stored on your computer). If you didn't manage to produce the dataset [use mine](https://github.com/ulfaslak/computational_analysis_of_big_data_2018_spring/tree/master/data) and go back and complete it at a later time so that you get the most out of this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.3.2**: Implement cross validation. The performance of a classifier is strongly dependent on the amount of data it is trained on. In Ex. 4.3.1 you train it on only half of the data and test it on the other half. If you rerun that code multiple times, with random 50/50 partitions, you are gonna see a lot of uncertainty in performance. Cross validation solves this problem by training on a larger subset of the data and testing on a smaller one, and taking the average performance over K-folds of this process.\n",
    "1. Implement cross validation over $K=10$ folds. For each fold you must record the training and test accuracies. In the end, visualize the distributions of test- and training accuracy as histograms in the same plot. It's important that you comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.4.1**: Let's put our classifier to use!\n",
    "* Retrain your model on all of your data.\n",
    "* Create a team alliance representation of the ambiguous characters\n",
    "* Use the model the estimate the probability that each character is a villain (let's call this *villainness*). You can use the `.predict_proba` method on the model to get probability estimates rather than class assignments.\n",
    "* **Visualize the \"heroness\" distribution for all ambiguous characters**. Comment on the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
