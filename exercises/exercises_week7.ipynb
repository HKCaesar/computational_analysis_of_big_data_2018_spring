{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `computational_analysis_of_big_data_2017` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thursday, October 19, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "The exercises today are about extracting high-level knowledge from text. We're still a long way from computers being able to give us insight as deep as that which we can aquire from manually reading text, but some the tools that you will use today get us a long way in understanding useful things about unreadibly large amount of text in comparatively little time.\n",
    "\n",
    "In the exercises today you will:\n",
    "\n",
    "* Create wordclouds\n",
    "* Extract sentiment from text\n",
    "* Construct a Bag of Words (BoW) matrix to represent how words are used about each faction in the Marvel dataset\n",
    "* Perform a TD-IDF transform to understand which words are important to different character factions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Science From Scratch* Chapter 20.\n",
    "\n",
    "You can brush over a lot of this stuff, but try to get a feel for the different concepts that are important in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they probably offer more buzz than usefulness, wordclouds are a fun way to get quick insight into text data. In this section you will generate one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.1.1**: To make a word cloud you need some more or less clean text. For each character extract as cleanly as you can, the text written on their wikipage. Since the wikidata is kind of messy, there are some things you should be aware of when extracting the text.\n",
    "* Exclude character names.\n",
    "* Exclude links.\n",
    "* Exclude numbers.\n",
    "* Set everything to lower case.\n",
    "* Do not include stopwords (use `nltk.corpus.stopwords.words(\"english\")` to get a list of stopwords; install `nltk` to do this).\n",
    "\n",
    ">Cleaning doesn't have to be perfect and can be done in a many different ways, these are just some things to look out for.\n",
    "\n",
    ">Once you have extracted the text, create one long text string for all text written about heroes, another long text string for villains, and finally one for ambiguous characters. Using the code snippet below which shows how to plot a word cloud, plot the word clouds for each faction.\n",
    "\n",
    ">        text = \"some cool text\"\n",
    ">        wc = wordcloud.WordCloud(max_font_size=40).generate(text)\n",
    ">        \n",
    ">        plt.figure()\n",
    ">        plt.imshow(wc, interpolation=\"bilinear\")\n",
    ">        plt.axis(\"off\")\n",
    ">        plt.show()\n",
    "\n",
    "> You have to import `matplotlib.pylab` and `wordcloud` to do this. You can install `wordcloud` with anaconda by typing into your console\n",
    "\n",
    ">        conda install -c https://conda.anaconda.org/amueller wordcloud\n",
    "\n",
    ">or\n",
    "\n",
    ">        sudo pip install wordcloud\n",
    "\n",
    ">if you don't have Anaconda installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you don't have time to read some text, but you need to know something about whether it positively or negatively toned. Enter *Sentiment Analysis*. The point of this exercise is to extract the sentiment of text on your heroes, villains and ambiguous characters and figure out whether Wikipedia is biased towards writing in a certain tone towards a certain kind of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.1**: For each of the three character class, compute the sentiment scores across wikipedia articles so that you can produce three histograms (one for each class) with sentiment scores. You can use the text strings you generated in the previous exercise. We will cheat a bit and use a library that does the scoring for us. Install `afinn` using `conda` or `pip` and extract the sentiment with that module. There's an example of how to use it on the library's [PyPi repository](https://pypi.python.org/pypi/afinn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, \"Bag of Words\" means breaking up a document into words and throwing them into a bag. And that's very close to the truth! In week 5 you constructed a \"team-affiliations\" matrix which had a row for each character and a column for each team. If the character was on a given team there would be a one for that character row at that team column, if not there would be a zero. The BoW is the same, only now, rather than teams, your columns are individual words that a character's wikipage might contain, and the numbers represent how many times those words appear.\n",
    "<img src=\"http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_bow.png\" width=\"400\"/>\n",
    "BoW's are pretty large and sparse (mostly contain zero's) matrices, but they are extremely useful because they allow us to use linear algebra to do things like PCA, classification, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.3.1**: If we were interested in learning about differences on the character level we would build a BoW matrix like the one in the image above. However, as you may have sensed, we care mostly about differences across the factions (heroes, villains, ambiguous). In this exercise, you will therefore construct a Bag of Words matrix that has three rows: one for each faction. There are functions that do this for you in `sklearn`, but today you will do it yourself. After you have done this, what is the shape of your matrix? I got `(3, 13087)`, but depending on how you process your data, you can get different results, which is all good. Also print the total sum of numbers inside this matrix, just so it's easier to check if you got within a reasonable range of correctness (remember that correctness is not binary anymore).\n",
    "\n",
    ">*Hints:*\n",
    "* *Since you already extracted lists of words for each faction in Ex. 7.1.1, you can use these to figure out what the total vocabulary of words used in your dataset is. You can \"clean up\" this vocabulary by a number of tricks. For example, there are tools for *stemming* words to remove grammar so that e.g. 'cat' and 'cats' both become 'cat', but that's all up to you whether you wanna go that deep.*\n",
    "* *You should also figure out which column should represent what word. To do this, you can create a single list with all words used for all characters, and take the `set` of it, so you obtain the vocabulary of your corpus. Then you can create a dictionary that maps from word to column index.*\n",
    "* *Now you should be able to loop over each faction's list of words and create vectors that represent how many times each vocabulary word was used within a given faction's wikipedia pages. Your final BoW matrix should be a `numpy` array which has shape `(3, vocabulary_size)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You didn't just make that BoW matrix for fun. We are interested in knowing how (or if) words are used differently across factions, and the best way to do that is to used something called a Term Frequency - Inverse Document Frequency (TF-IDF) transformation. You can read about it [on Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), but the gist is that it reflects how important each word is to each document (a document in this case being a faction).\n",
    "\n",
    "It works in two steps:\n",
    "* (1 - TF) you normalize over the frequency of each word in each document, so that rows sum to 1. Every row is now a probability distribution (a pmf to be exact) that gives the \"term frequency\" in each document.\n",
    "* (2 - IDF) you weigh the TF by the inverse document frequency, which measures how unique a word to specific documents. For example, the word \"the\" will be frequently used in every document (high TF) but we know it's not very special because it's used in all documents so the inverse document frequency is low, yielding a vanishing TF-IDF score for \"the\" in all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.4.1**: Use any tool you like (you can do it manually, it's not impossible), to perform a TF-IDF transform on your BoW matrix from Ex. 7.3.1. The result should be a matrix of the same shape as the BoW, but with different values inside.\n",
    "1. What do these values reflect?\n",
    "2. Report the top 10 words for each faction.\n",
    "3. Comment on the results. From what you learned when you plotted the word cloud, is this the expected result?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
