{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `computational_analysis_of_big_data_2017` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thursday, October 5, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "To apply machine learning and get some initial results is fairly straight forward. So is understanding understanding how machine learning models actually work. There are innumerable machine learning models (and algorithms for fitting them to data) out there, and each one has something special about them that makes them suitable to a specific type of problem. However, most of them are just mutations of simple machinery like the decision tree, linear regression, bayesian inference and density estimation. This week we will open some of these machines and look at how they work.\n",
    "\n",
    "In the exercises today you will:\n",
    "\n",
    "* Use PCA to visualize your data\n",
    "* Implement a standard decision tree mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Science From Scratch* Chapter 13, 14, 15, 16, 17, (18), 19.\n",
    "\n",
    "This looks like a lot of reading – BUT – rather than reading it all in depth and understanding everything you should just brush through and familiarize yourself with the concepts. Look for answers to these questions:\n",
    "\n",
    "* What is it that the Naive Bayes classifier does?\n",
    "* What is the difference between simple linear regression and multiple regression?\n",
    "* Why would we rather use a logistic regression rather than a linear regression for classification?\n",
    "* What is the basic idea behind decision trees?\n",
    "* What is clustering and how is it different from all these other supervised methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get started**: Load the data you used last week to classify hero/villainness. It should consist of two arrays, one of dimensions ($N_{characters}$, $N_{teams}$) which is your feature matrix that has the team affiliations of each character as a row vector of ones and zeros, and another of dimensions ($N_{characters}$, ) which is your target array that gives whether a character is a hero or a villain. You can either load the data or copy/paste the code that generates it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this exercise is to learn how to visualize high-dimensional data, and get a feeling of how your Marvel data looks when projected to a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 6.1.1**: Apply a PCA to your data. If `X` is your feature matrix, a PCA can be done like:\n",
    "\n",
    ">        pca = sklearn.decomposition.PCA()\n",
    ">        pca.fit(X)\n",
    ">        X_pca = pca.transform(X)\n",
    "\n",
    ">1. What is the dimensionality of `X_pca` compared to `X`? What happened to `X` when you transformed it?\n",
    "2. Plot the first two components/columns of the transformed data and color the points by their class label. My plot looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.1.1.2.png). Comment on the result. What would plotting two other components against each other show you?\n",
    "3. Plot the explained variance ratio of each component. What does this tell you about the dataset? My plot looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.1.1.3.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the decision making algorithm of a decision tree classifier, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 6.2.1**: Read about Shannon entropy.\n",
    "1. What is it?\n",
    "2. Write a function that computes the Shannon-entropy of a probability vector. Compute the Shannon entropy of `p=[0.4, 0.6]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 6.2.2**: Split your data into two subsets. One where characters are affiliated with X-men and one where they are not.\n",
    "1. What is the entropy of target labels in each subset?\n",
    "2. What is the weighted average entropy of the split?\n",
    "3. Write a function that computes the weighted average entropy of a split, given the data and team (name or id) on which to split the data.\n",
    "4. Plot the distribution of split entropy for all features. Comment on the result. My figure looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.2.4.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 6.2.3**: Print the maximum entropy path of a decision tree.\n",
    ">\n",
    ">1. Implement the following pseudocode and print the output:<br><br>\n",
    ">Step 1. Find `team` that gives lowest split entropy for `data`. Print `team`.<br>\n",
    ">Step 2. Split `data` on `team`, to produce `data0` and `data1`. Print the entropy of each, as well as their weighted avg. entropy.<br>\n",
    ">Step 3. Overwrite the `data` variable with either `data0` or `data1`, depending on which has the highest entropy.<br>\n",
    ">Step 4. Stop if there are less than 5 datapoints in `data`. Otherwise start over from 1.<br><br>\n",
    ">My output looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.3.1.png) for the first five splits.<br><br>\n",
    ">\n",
    ">2. Comment on decision path your code takes: How splits are there? Do you notice anything interesting about the final splits? Why do we choose to stop splitting before `data` get smaller than 5?\n",
    ">3. Train a `sklearn.tree.DecisionTreeClassifier` classifier on the dataset. Initiate the classifier with `criterion='entropy'`. What are the most important features of this classifier? How does this line up with the order of the order of splits you just printed (a comment is fine)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
